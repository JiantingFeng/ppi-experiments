{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 10000\n",
    "n_dims = 10\n",
    "\n",
    "X = np.random.randn(n_samples, n_dims)\n",
    "\n",
    "beta_true = np.asarray([0]*(n_dims // 2) + [1] * (n_dims - n_dims//2))\n",
    "\n",
    "# Generate target variable\n",
    "y = np.random.binomial(1, 1 / (1 + np.exp(-X.dot(beta_true))))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "X_train_labeled, X_train_unlabeled, y_train_labeled, _ = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
    "X_valid, X_holdout, y_valid, y_holdout = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1pexp(x):\n",
    "    \"\"\"\n",
    "    Numerically accurate evaluation of log(1 + exp(x)).\n",
    "    \n",
    "    This function computes log(1 + exp(x)) in a way that is numerically stable\n",
    "    for both large positive and large negative values of x.\n",
    "    \n",
    "    Args:\n",
    "    x (np.ndarray): Input array\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: log(1 + exp(x)) computed in a numerically stable way\n",
    "    \"\"\"\n",
    "    # For large positive x, log(1 + exp(x)) ≈ x\n",
    "    # For x near zero, we use the direct computation\n",
    "    # For large negative x, we use exp(x) to avoid overflow\n",
    "    threshold = np.log(np.finfo(x.dtype).max) - 1e-5\n",
    "    \n",
    "    result = np.where(\n",
    "        x > threshold,\n",
    "        x,\n",
    "        np.where(\n",
    "            x > -threshold,\n",
    "            np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0),\n",
    "            np.exp(x)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Numerically stable sigmoid function.\n",
    "\n",
    "    This function computes the sigmoid of x in a way that is numerically stable\n",
    "    for both large positive and large negative values of x.\n",
    "\n",
    "    Args:\n",
    "    x (np.ndarray): Input array\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Sigmoid of x computed in a numerically stable way\n",
    "    \"\"\"\n",
    "    # For large negative x, sigmoid(x) ≈ exp(x)\n",
    "    # For large positive x, sigmoid(x) ≈ 1 - exp(-x)\n",
    "    # For x near zero, we use the standard formula\n",
    "    \n",
    "    mask = x >= 0\n",
    "    z = np.exp(-np.abs(x))\n",
    "    \n",
    "    return np.where(mask, 1 / (1 + z), z / (1 + z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(X, y, temp, beta):\n",
    "    # X: [n, d], beta: [d]\n",
    "    z = X @ beta / temp\n",
    "    loss = np.mean(-y * z + log1pexp(z))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logistic_loss(X, y, temp, beta):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of logistic loss with respect to beta.\n",
    "    \n",
    "    Args:\n",
    "    X (np.ndarray): Feature matrix (n_features, n_samples)\n",
    "    y (np.ndarray): Target vector (n_samples,)\n",
    "    beta (np.ndarray): Coefficient vector (n_features,)\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Gradient vector (n_features,)\n",
    "    \"\"\"\n",
    "    z = (X @ beta) / temp\n",
    "    sigmoid_z = sigmoid(z)\n",
    "    grad_loss = (X.T @ (sigmoid_z - y)) / len(y)\n",
    "    return grad_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_logistic_regression(X, y, temp: float=1):\n",
    "    \"\"\"\n",
    "    Optimize the logistic regression parameters using L-BFGS.\n",
    "    \n",
    "    Args:\n",
    "    X (np.array): Feature matrix (N x D)\n",
    "    y (np.array): True binary labels (N,)\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Optimized parameters\n",
    "    \"\"\"\n",
    "    # Initialize beta (parameters)\n",
    "    initial_beta = np.zeros(X.shape[1])\n",
    "    loss_func = partial(logistic_loss, X, y, temp)\n",
    "    grad_func = partial(grad_logistic_loss, X, y, temp)\n",
    "    print(initial_beta.shape)\n",
    "    # Optimize using L-BFGS\n",
    "    result = minimize(\n",
    "        fun=loss_func,\n",
    "        x0=initial_beta,\n",
    "        method='BFGS',\n",
    "        jac=grad_func,\n",
    "        options={'disp': True}\n",
    "    )\n",
    "\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train @ np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.428228\n",
      "         Iterations: 15\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n"
     ]
    }
   ],
   "source": [
    "beta_hat = optimize_logistic_regression(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03632113, -0.04375023, -0.0967291 ,  0.00931281,  0.0899625 ,\n",
       "        1.07567776,  1.00714916,  0.89664905,  1.01231026,  0.96359944])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppi_logistic_loss(X_labeled, y_labeled, X_unlabeled, beta_hat, temp, beta):\n",
    "    loss_unlabeled = logistic_loss(X_unlabeled, X_unlabeled @ beta_hat, temp, beta)\n",
    "    loss_correction = logistic_loss(X_labeled, y_labeled, temp, beta) - logistic_loss(X_labeled, X_labeled @ beta_hat, temp, beta)\n",
    "    return loss_unlabeled - loss_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_ppi_logistic_loss(X_labeled, y_labeled, X_unlabeled, beta_hat, temp, beta):\n",
    "    grad_unlabeled = grad_logistic_loss(X_unlabeled, X_unlabeled @ beta_hat, temp, beta)\n",
    "    grad_correction = grad_logistic_loss(X_labeled, y_labeled, temp, beta) - grad_logistic_loss(X_labeled, X_labeled @ beta_hat, temp, beta)\n",
    "    return grad_unlabeled + grad_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ppi_logistic_regression(X_labeled, y_labeled, X_unlabeled, temp: float=1):\n",
    "    \"\"\"\n",
    "    Optimize the logistic regression parameters using L-BFGS.\n",
    "    \n",
    "    Args:\n",
    "    X_labeled (np.array): Labeled feature matrix\n",
    "    y_labeled (np.array): Labels for labeled data\n",
    "    X_unlabeled (np.array): Unlabeled feature matrix\n",
    "    temp (float): Temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Optimized parameters\n",
    "    \"\"\"\n",
    "    # Initialize beta (parameters)\n",
    "    initial_beta = np.zeros(X_labeled.shape[1])\n",
    "\n",
    "    beta_hat = optimize_logistic_regression(X_holdout, y_holdout, temp) # Only use once, then discard this part of data\n",
    "\n",
    "    loss_func = partial(ppi_logistic_loss, X_labeled, y_labeled, X_unlabeled, beta_hat, temp)\n",
    "    grad_func = partial(grad_ppi_logistic_loss, X_labeled, y_labeled, X_unlabeled, beta_hat, temp)\n",
    "    \n",
    "    # Optimize using BFGS\n",
    "    result = minimize(\n",
    "        fun=loss_func,\n",
    "        x0=initial_beta,\n",
    "        method='BFGS',\n",
    "        jac=grad_func,\n",
    "        options={'disp': True}\n",
    "    )\n",
    "\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.428228\n",
      "         Iterations: 15\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "         Current function value: -5.224959\n",
      "         Iterations: 7\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianting/miniconda3/lib/python3.12/site-packages/scipy/optimize/_minimize.py:726: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
     ]
    }
   ],
   "source": [
    "beta_ppi = optimize_ppi_logistic_regression(X_train_labeled, y_train_labeled, X_train_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40178529,  0.01404516,  0.21538011, -0.22745011, -0.40105692,\n",
       "        0.83671883,  0.98146178,  0.66464789,  0.2080279 ,  0.83749   ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
